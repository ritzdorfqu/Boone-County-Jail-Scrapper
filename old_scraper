from bs4 import BeautifulSoup
from datetime import timedelta
import requests
import requests
import requests_cache
from time import sleep
from models import Info

requests_cache.install_cache(
    'cache',
    expire_after=timedelta(hours=24),
    allowable_methods=('GET', 'POST')
)

SEARCH_URL = 'https://report.boonecountymo.org/mrcjava/servlet/RMS01_MP.I00030s?max_rows=10000'


def get_search_results():
    r = requests.get(
        SEARCH_URL,
        headers={'user-agent': "I'm good people!!!"}
    )
    soup = BeautifulSoup(r.content)

    r.raise_for_status()

    return r.content


def extract_details_url(search_results):
    soup = BeautifulSoup(search_results)

    details_urls = []

    table = soup.find('table', class_='collapse data-table shadow responsive')

    tr_all = table.find_all('tr', class_=['odd', 'even'])

    for tr in tr_all:
        td_all = tr.find_all('td')
        url = td_all[9].find('a').attrs['href']
        details_urls.append(url)

    return details_urls


def get_details(rel_url):

    full_url = 'https://report.boonecountymo.org/mrcjava/servlet/' + rel_url

    r = requests.post(
        full_url,
        headers={'user-agent': "I'm good people!!!"}
    )

    r.raise_for_status()

    return r.content


def extract_details_data(details):
    soup = BeautifulSoup(details)

    tables = soup.find_all('table', class_='collapse centered_table shadow')

    detainee_info_table = tables[0]

    detainee_info_cells = detainee_info_table.find_all(
        'td', class_='two td_left'
    )

    info = Info.create(
        height=detainee_info_cells[0].text.strip(),
        weight=detainee_info_cells[1].text.strip(),
        sex=detainee_info_cells[2].text.strip(),
        eyes=detainee_info_cells[3].text.strip(),
        hair=detainee_info_cells[4].text.strip(),
        race=detainee_info_cells[5].text.strip(),
        age=detainee_info_cells[6].text.strip(),
        city=detainee_info_cells[7].text.strip(),
        state=detainee_info_cells[8].text.strip(),
    )


def main():
    search_results = get_search_results()
    urls = extract_details_url(search_results)

    for url in urls:
        print('extracting data for %s' % url)
        details = get_details(url)
        extract_details_data(details)
        print('data for %s' % url)
        sleep(3)

    sleep(3)


if __name__ == '__main__':
    main()
